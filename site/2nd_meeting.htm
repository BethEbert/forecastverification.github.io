<HTML>
<HEAD>
<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=windows-1252">
<META NAME="Generator" CONTENT="Microsoft Word 97">
<TITLE>Notes from 1st meeting of the Verification Group, 3 July 2002, Boulder</TITLE>
</HEAD>
<BODY LINK="#0000ff">

<B><FONT SIZE=4><P ALIGN="CENTER">2<SUP>nd</SUP> meeting of the Forecast Verification Group</P>
<P ALIGN="CENTER">5 September 2002, University of Reading</P>
</B></FONT>
<B><P>Present</B>: Barbara Casati, Ulrich Damrath, Beth Ebert, Anna Ghelli, Martin Goeber, Pertti Nurmi, David Stephenson, Clive Wilson, Laurie Wilson</P>

<B><P>Background:</P>

<UL>
</B><LI>At the NCAR Workshop on &quot;Making Verification More Meaningful&quot; in August there was some agreement that it would be a good idea to put together a web page on how to verify forecasts</LI>
<LI>The Sydney 2000 FDP verification team met after the workshop, decided to do a web page with methods and FAQs and sample data</LI>
<LI>BE volunteered to draft something to be circulated, group’s aim to have something available (even if incomplete) in time for the Reading QPF Conference</LI></UL>


<B><P>URL (temporary):</P>
</B><P><A HREF="http://www.bom.gov.au/bmrc/wefor/staff/eee/verif/verif_web_page.html">http://www.bom.gov.au/bmrc/wefor/staff/eee/verif/verif_web_page.html</A></P>

<B><P>Discussion of web page contents:</P>

<UL>
</B><LI>Need well-defined limits of what the site contains and doesn’t contain.</LI>
<LI>Instead of pros and cons, which imply judgement, the methods should include summaries of their <I>characteristics.</LI>
</I><LI>A list of cautions, possibly presented as commandments (e.g., &quot;Thou shalt not verify NWP forecasts using NWP analyses&quot;) would be useful.</LI>
<LI>LW thought that the Stanski et al (1989) text could be put onto a web page that we could link to.</LI>
<LI>DS said that the glossary and bibliography of his forthcoming book could possibly be made accessible via the web, and we could link to it.</LI>
<LI>A few verification datasets that include both forecasts and verifying observations (e.g. Finlay tornados, Sydney 2000 nowcasts, something from climate world, etc.) should be downloadable from our site, also used to demonstrate verification methods.</LI>
<LI>Examples should be friendly, and should include interpretation along with the results (same page, or linked on a separate page?)</LI>
<LI>DS suggested reorganising the verification methods by (a) deterministic forecasts, (b) probabilistic forecasts. Note that forecasts can be transformed from one type to another.</LI>
<LI>Diagnostic methods should have a very brief description on our page, then a link or reference to more detailed information elsewhere.</LI></UL>


<B><P>Discussion of useability issues (which may require technical help):</P>

<UL>
</B><LI>For feedback and interaction it would be a good idea to have a subscribe-able mailing list.</LI>
<LI>Need some way to search the site, maybe also a site map, which would make navigation easier.</LI>
<LI>Would be very nice to have a forecast flowchart such as the one from Stanski et al., with clickable links to appropriate verification methods.</LI>
<LI>Verification datasets should be FTP-able from our page.</LI></UL>


<B><P>Actions:</P>
</B><I>
<P>Everyone:</P>

<UL>
</I><LI>Provide links to other good sites with verification and statistics information</LI>
<LI>Provide references to useful verification papers, reports, and other publications</LI>
<LI>Think of important issues, and if possible , write a paragraph or addressing each (if you want to discuss any issue at greater length it might be better to have a separate page).</LI>
<LI>Think of relevant FAQs</LI>
<LI>Make web pages for your own diagnostic verification methods, that we can link to</LI>
<LI>Write short (1 paragraph or less) answers to FAQs, use links and/or references for more details</LI></UL>
<DIR>

<P>LW - 1. How many samples are needed to get reliable verification results?</P>
<P>BE - 2. What is the best statistic for measuring the accuracy of a forecast?</P>
<P>CW &amp; MG - 3. Why does a more realistic-looking, detailed, forecast appear to perform worse than a smoother, less realistic-looking forecast?</P>
<P>BE - 4. What is a "double penalty"?</P>
<P>AG - 5. How do I compare gridded forecasts (from a model, for example) with observations at point locations?</P>
<P>AG - 6. How do I verify worded forecasts?</P>
<P>LW - 7. What does "hedging" a forecast mean?</P>
<P>DS - 8. What does "strictly proper" mean when referring to verification statistics?</P>
<P>UD - 9. Is there a difference between "verification" and "validation"?</P>
<P>More are welcome!</P>
</DIR>

<I><P>BE:</I> continue to administer and update this web site. Assemble e-mail list for this group, create a subscribe-able mailing list for the broader verification community.</P>

<P>It would be a good idea if all (non-trivial) web site business goes to everyone in the group so we all know what’s happening.</P>

<P>Thanks to Barbara Casati for organising the meeting room and lunch</P></BODY>
</HTML>
