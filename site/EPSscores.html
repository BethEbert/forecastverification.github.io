<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
<html>
<head>
   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
   <meta name="generator" content="HTML Tidy, see www.w3.org">
   <meta name="Author" content="Beth Ebert">
   <meta name="GENERATOR" content="Mozilla/4.79 [en] (Windows NT 5.0; U) [Netscape]">
   <title>Forecast Verification - Methods and FAQ</title>
   <link rel = "stylesheet" href="../default.css">
<style type="text/css">
  H1 {font-size: x-large; color: #993366; font-family: Arial,Helvitica, sans-serif; font-style: italic }
  H2 {font-size: large; color: #993366; font-family: Arial,Helvitica, sans-serif; font-style: italic }
  H3 {font-size: medium; color: black; font-family: Arial,Helvitica, sans-serif; font-style: italic }
  H4 {font-size: small; color: black; font-family: Arial,Helvitica, sans-serif; font-style: italic }
  p  {font-size: medium; color: black; font_family: Times, serif; font-style: normal }
 body {color: #000000;}
 :link { color: #0000EE }
 :visited { color: #551A8B }
 :active { color: #FF0000 }
</style>
<!--  p  {font-size: small; color: black; font-family: Arial,Helvitica, sans-serif; font-style: normal } -->
<script type="text/javascript">
  <!--
  function popup(mylink, windowname)
  {
  if (! window.focus)return true;
  var href;
  if (typeof(mylink) == 'string')
     href=mylink;
  else
     href=mylink.href;
  window.open(href, windowname, 'width=400,height=200,dependent=yes,left=300,top=200resizable=yes,scrollbars=yes');
  return false;
  }
  //-->
  </script>
<script type="text/javascript">
  <!--
  function dropdown(mySel)
  {
  var myWin, myVal;
  myVal = mySel.options[mySel.selectedIndex].value;
  if(myVal)
     {
     if(mySel.form.target)myWin = parent[mySel.form.target];
     else myWin = window;
     if (! myWin) return true;
     myWin.location = myVal;
     }
  return false;
  }
  //-->
</script>
</head>
<body>

<h1>
<b><i><font color="#666600">Methods and scores used for verifying ensemble
forecasts</font></i></b></h1>
<b><font color="#666600">Short cuts to:</font></b>
<br><font color="#666600"><a href="#BIAS">Bias score</a></font>
<br><font color="#666600"><a href="#ETS">Equitable threat score</a></font>
<br><font color="#666600"><a href="#RMSE">Root mean square error</a></font>
<br><font color="#666600"><a href="#Skill score">Generalised skill score</a></font>
<br><font color="#666600"><a href="#reliability">Reliability diagram</a></font>
<br><font color="#666600"><a href="#BSS">Brier skill score</a></font>
<br><font color="#666600"><a href="#ROC">Relative operating characteriscis
(ROC)</a></font>
<br><font color="#666600"><a href="#RPSS">Ranked probability skill score</a></font>
<br><font color="#666600"><a href="#rank histogram">Rank histogram</a></font>
<p><font color="#666600">These descriptions are taken from the WWRP-WGNE
Joint Verification Working Group's <a href="../index.html">verification
web site</a> and expanded upon in some cases using extracts lifted unashamedly
from Stanski et al. (1989).</font>
<br>
<hr WIDTH="100%">
<h2>
<b><i>Verification of non-probabilistic forecasts</i></b></h2>

<h3>
<b><i>(a) Dichotomous forecasts</i></b></h3>
A dichotomous forecast says, "yes, an event will happen", or "no, the event
will not happen". In the case of ensemble verification we usually verify
the forecast occurrence of an event greater than a certain threshold (for
example, daily rainfall of at least 1 mm/day). Because these apply to deterministic
(non-probabilistic) forecasts, we use them to verify the individual ensemble
members as well as the ensemble mean.
<p>To verify this type of forecast&nbsp; we start with a contingency table
that shows the frequency of "yes" and "no" forecasts and occurrences. The
four combinations of forecasts (yes or no) and observations (yes or no),
called the
<i>conditional distribution</i>, are:
<p>&nbsp;&nbsp;&nbsp;&nbsp; <i>hit</i> - event forecast to occur, and did
occur
<br>&nbsp;&nbsp;&nbsp;&nbsp; <i>miss</i> - event forecast not to occur,
but did occur
<br>&nbsp;&nbsp;&nbsp;&nbsp; <i>false alarm</i> - event forecast to occur,
but did not occur
<br>&nbsp;&nbsp;&nbsp;&nbsp; <i>correct negative</i> - event forecast not
to occur, and did not occur
<p>The total numbers of forecast and observed occurrences and non-occurences
are given on the lower and right sides of the contingency table, and are
called the <i>marginal distribution</i>.
<br>&nbsp;
<br>&nbsp;
<table WIDTH="500" HEIGHT="180" summary="Two by two
contingency table of forecasts and observations" >
<caption><b>Contingency Table</b></caption>

<tr ALIGN=CENTER VALIGN=CENTER>
<td><b>&nbsp;</b></td>

<td><b>&nbsp;</b></td>

<td WIDTH="140"></td>

<td ALIGN=LEFT WIDTH="140">&nbsp;Forecast</td>

<td WIDTH="140"></td>
</tr>

<tr ALIGN=CENTER VALIGN=CENTER>
<td>&nbsp;</td>

<td>&nbsp;</td>

<td>yes</td>

<td>no</td>

<td>&nbsp;Total</td>
</tr>

<tr ALIGN=CENTER VALIGN=CENTER>
<td ALIGN=LEFT VALIGN=BASELINE>Observed</td>

<td>yes</td>

<td BGCOLOR="#33FF33"><b><i>hits</i></b></td>

<td BGCOLOR="#33FF33"><b><i>misses</i></b></td>

<td BGCOLOR="#99FF99"><b><i>observed yes</i></b></td>
</tr>

<tr ALIGN=CENTER VALIGN=CENTER>
<td><b>&nbsp;</b></td>

<td>no</td>

<td BGCOLOR="#33FF33"><b><i>false alarms</i></b></td>

<td BGCOLOR="#33FF33"><b><i>correct negatives</i></b></td>

<td BGCOLOR="#99FF99"><b><i>observed no</i></b></td>
</tr>

<tr ALIGN=CENTER VALIGN=CENTER>
<td ALIGN=LEFT>Total</td>

<td><b>&nbsp;</b></td>

<td BGCOLOR="#99FF99"><b><i>forecast yes</i></b></td>

<td BGCOLOR="#99FF99"><b><i>forecast no</i></b></td>

<td BGCOLOR="#99FF99"><b><i>total</i></b></td>
</tr>
</table>

<p>The contingency table is a useful way to see what types of errors are
being made. A perfect forecast system would produce only
<i>hits</i> and
<i>correct
negatives</i>, and no <i>misses</i> or <i>false alarms</i>.
<p>There are several categorical statistics that can be computed from the
yes/no contingency table. The ones used in the EPS verification are:
<br>&nbsp;
<p><a NAME="BIAS"></a><b><i>Bias score</i></b> -&nbsp;<img SRC="BIAS.gif" ALT="Equation for bias" height=53 width=194 align=CENTER>
<p>Measures the ratio of the frequency of forecast events to the frequency
of observed events. It measures the ability to forecast events with the
same frequency as found in the sample, without regard to forecast accuracy.
<p><b>Range:</b> 0 to infinity.&nbsp; <b>Perfect score:</b> 1.
<p><b>Characteristics:</b> Indicates whether the forecast system has a
tendency to underforecast (<i>BIAS</i>&lt;1) or overforecast (<i>BIAS</i>>1)
events. Does not measure how well the forecast corresponds to the observations,
only measures relative frequencies.
<br>&nbsp;
<p><a NAME="ETS"></a><b><i>Equitable threat score</i></b>-<img SRC="ETSa.gif" ALT="Equation for equitable threat score" height=60 width=339 align=CENTER>
where&nbsp;<img SRC="ETSb.gif" ALT="Equation for hits due to random chance" height=47 width=332 align=CENTER>
<p>Measures the fraction of observed and/or forecast events that were correctly
predicted, adjusted for hits associated with random chance. This score
is often used in the verification of rainfall in NWP models because its
"equitability" allows scores to be compared more fairly across different
regimes.
<p><b>Range:</b> -1/3 to 1, 0 indicates no skill.&nbsp;&nbsp; <b>Perfect
score:</b> 1.
<p><b>Characteristics:</b> Sensitive to hits, penalizes both misses and
false alarms, accounts for climatological event frequency. Does not distinguish
source of forecast error.
<br>&nbsp;
<br>&nbsp;
<h3>
<b><i>(b) Continuous forecasts</i></b></h3>

<p><br>Verification of forecasts of continuous variables measures how the
<i>values</i>
of the forecasts differ from the <i>values</i> of the observations.
<br>&nbsp;
<p><a NAME="RMSE"></a><b><i>Root mean square error</i></b> -&nbsp;<img SRC="RMSE.gif" ALT="Equation for root mean square error" height=58 width=178 align=CENTER>
<p>Measures "average" error, weighted according to the square of the error.
The root mean square error puts greater influence on large errors than
smaller errors, which may be a good things if large errors are especially
undesirable.
<p><b>Range:</b> 0 to infinity.&nbsp; <b>Perfect score:</b> 0.
<p><b>Characteristics:</b> Simple, familiar. Does not indicate the direction
of the deviations. Emphasis on large errors may encourage conservative
forecasting.
<br>&nbsp;
<br>&nbsp;
<p><a NAME="Skill score"></a><b><i>Generalised skill score</i></b> -&nbsp;<img SRC="Skill.gif" ALT="Equation for equation for skill score" height=51 width=310 align=CENTER>
<p>Measures improvement over a reference forecast. When
<i>MSE</i> is the
score used in the above expression then the resulting statistic is called
the <b>reduction of variance</b>.
<p><b>Range:</b> minus infinity to 1, 0 indicates no improvement in skill
over the reference forecast. <b>Perfect score:</b> 1.
<p><b>Characteristics:</b> Implies information about the value or worth
of a forecast relative to an alternative (reference) forecast. Can be unstable
for small sample sizes.
<p>
<hr WIDTH="100%">
<h2>
<b><i>Verification of probabilistic forecasts</i></b></h2>
A probabilistic forecast gives a <i>probability</i> of an event occurring,
with a value between 0 and 1 (or 0 and 100%). It is impossible to verify
a single probabilistic forecast using a single observation. Instead one
must verify a <i>set</i> of probabilistic forecasts, <i>p<sub>i</sub></i>,
using observations that those events either occurred (<i>o<sub>i</sub></i>=1)
or did not occur (<i>o<sub>i</sub></i>=0).
<p>A good probability forecast system has several attributes:
<p>&nbsp;&nbsp;&nbsp;&nbsp; <i>reliability</i> - agreement between forecast
probability and mean observed frequency; like a categorical bias
<br>&nbsp;&nbsp;&nbsp;&nbsp; <i>sharpness</i> - tendency to forecast extreme
values -- "climatology" is not sharp
<br>&nbsp;&nbsp;&nbsp;&nbsp; <i>resolution</i> - ability of forecast to
resolve the set of sample events into subsets with characteristically different
frequencies
<p><a NAME="reliability"></a><img SRC="ReliabilityDiagram.gif" ALT="Reliability diagram of observed frequency vs forecast probability" height=319 width=333 align=RIGHT><b><i>Reliability
diagram</i></b> - (also called "attributes diagram").
<br>The reliability diagram plots the observed frequency against the forecast
probability, where the range of forecast probabilities is divided into
<i>K</i>
bins (for example, 0-5%, 5-15%, 15-25%, etc.). The diagonal line indicates
perfect reliability (average observed frequency equal to predicted probability
for each category), and the horizontal line represents the climatological
frequency. Sometimes sample sizes are plotted either as a histogram, or
as numbers next to the data points.
<p>The reliability diagram is in some ways analagous to a scatter plot,
where the data are stratified by the forecast into <i>K</i> categories
(<i>K</i> points). The reliability diagram thus represents stratification
conditioned on the forecast and can be expected to give information on
the real meaning of the forecast. Reliability is indicated by the proximity
of the plotted curve to the diagonal. The deviation from the diagonal gives
the <i>conditional bias</i>. If the curve lies below the line, this indicates
overforecasting (probabilities too high); points above the line indicate
underforecasting (probabilities too low).
<p>Sharpness may be inferred from the distribution of sample sizes in the
probability categories. The sharper the forecast, the more precise the
forecasts are and the greater the sample sizes in the extreme categories
near 0% and 100%. Conservative (not sharp) forecasting is indicated by
large percentages of the sample occurring in the probability categories
near the climatological probability. Sharpness does not guarantee a good
forecast unless it is also accurate. For a given level of reliability,
the sharper the forecast the more informative it will be.
<p>Resolution indicates the ability of the forecast to separate the events
from the non-events. A biased forecast may still have good resolution,
which means that it may be possible to improve the forecast through calibration.
The flatter the curve in the reliability diagram, the less resolution it
has. A forecast of climatology does not discriminate at all between events
and non-events, and thus has no resolution.
<p>The reliability diagram is conditioned on the observations (i.e., given
that Y occurred, what was the performance of the forecast?). It it a good
partner to the ROC, which is conditioned on the forecasts.
<br>&nbsp;
<p><a NAME="BS"></a><b><i>Brier score</i></b> -&nbsp;<img SRC="BSexpanded.gif" ALT="Expansion of Brier Score" height=47 width=473 align=CENTER>
<p>The Brier score is the mean squared error in probability space. <a href="#Murphy 1973">Murphy
(1973)</a> showed that it could be partitioned into three terms: (1) <i>reliability</i>,
(2)
<i>resolution</i>, and (3) <i>uncertainty</i>.
<p><b>Range:</b> 0 to 1.&nbsp; <b>Perfect score:</b> 0.
<p>The Brier score is sensitive to climatological frequency of the event.
In the absence of any forecasting skill, the best strategy to optimise
the Brier score is to forecast the climatological frequency. The more rare
an event, the easier it is to get a good <i>BS</i> without having any real
skill. For this reason, the Brier skill score (see below) is preferred
because it references the score to climatology (sample or long-term).
<br>&nbsp;
<p><a NAME="BSS"></a><b><i>Brier skill score</i></b> -&nbsp;<img SRC="BSS.gif" ALT="Equation for Brier skill score" height=54 width=274 align=CENTER>
<p>Measures the improvement of the probabilistic forecast relative to a
reference forecast (usually the long-term or sample climatology)
<p><b>Range:</b> minus infinity to 1, 0 indicates no skill when compared
to the reference forecast. <b>Perfect score:</b> 1.
<p><b>Characteristics:</b> Takes climatological frequency into account.
Because the denominator approaches 0 for a perfect forecast, this score
can be unstable when applied to small data sets. This score should always
be applied to a sufficiently large sample, one for which the sample climatology
of the event is representative of the long term climatology. The rarer
the event, the larger the number of samples needed to stablise the score.
For best results the Brier skill score should be computed on the whole
sample, i.e., the skill should be computed for an aggregated sample, not
averaged for several samples.
<br>&nbsp;
<p><a NAME="ROC"></a><b><i>Relative operating characteristic</i></b> -&nbsp;<img SRC="ROC.gif" ALT="Relative operating characteristic (ROC)" height=299 width=292 align=RIGHT>Plot
<i>probability
of detection</i> vs <i>false alarm rate</i> (<i>false alarms</i> / <i>observed
no</i>, also known as
<i>probability of false detection</i>), using a set
of increasing probability thresholds (for example, 0.05, 0.15, 0.25, etc.)
to make the yes/no decision. The area under the ROC curve is frequently
used as a score.
<p>ROC measures the ability of the forecast to discriminate between two
alternative outcomes.&nbsp;
<p><b>Range:</b> 0 to 1, 0.5 indicates no skill. <b>Perfect score:</b>
1
<p><b>Characteristics:</b> A good ROC is indicated by a curve that goes
close to the upper left corner (low false alarm rate, high probability
of detection). It is not sensitive to bias in the forecast, so says nothing
about reliability. The ROC is conditioned on the forecasts (i.e., given
a forecast of X, what was the resulting performance?) It allows the cost
of false alarms to be assessed, but does not deal with missed events. It
it a good partner to the reliability diagram, which is conditioned on the
observations.
<p><a NAME="RPS"></a><b><i>Ranked probability score</i></b> -&nbsp;<img SRC="RPS.gif" ALT="ranked probability score formula" height=58 width=251 align=CENTER>
<br>where <i>M</i> is the number of forecast categories,
<i>p<sub>k</sub></i>
is the predicted probability in forecast category <i>k</i>, and <i>o<sub>k</sub></i>
is an indicator (0=no, 1=yes) for the observation in category <i>k</i>.
<p>Measures the sum of squared differences in cumulative probability space
for a multi-category probabilistic forecast.
<p><b>Range:</b> 0 to 1.&nbsp; <b>Perfect score:</b> 0.
<p><b>Characteristics:</b> The <i>RPS</i> penalizes forecasts less severely
when their probabilities are close to the true outcome, and more severely
when their probabilities are further from the actual outcome. For two forecast
categories the <i>RPS</i> is the same as the Brier Score.
<p><a NAME="RPSS"></a><b><i>Ranked probability skill score</i></b> -&nbsp;<img SRC="RPSS.gif" ALT="Equation for ranked probability skill score" height=50 width=326 align=CENTER>
<p>Measures the improvement of the multi-category probabilistic forecast
relative to a reference forecast (usually the long-term or sample climatology)
<p><b>Range:</b> minus infinity to 1, 0 indicates no skill when compared
to the reference forecast. <b>Perfect score:</b> 1.
<p><b>Characteristics:</b> The RPSS is similar to that for the 2-category
Brier skill score, i.e., it takes climatological frequency into account.
Because the denominator approaches 0 for a perfect forecast, this score
can be unstable when applied to small data sets. This score should always
be applied to a sufficiently large sample, one for which the sample climatology
of the event is representative of the long term climatology. The rarer
the event, the larger the number of samples needed to stablise the score.
For best results the Brier skill score should be computed on the whole
sample, i.e., the skill should be computed for an aggregated sample, not
averaged for several samples.
<br>&nbsp;
<p><a NAME="rank histogram"></a><b><i>Rank histogram</i></b> (<a href="#Hamill 2001">Hamill,
2001</a>)<img SRC="RankHistogram.gif" ALT="Rank histogram" height=119 width=379 align=ABSCENTER>
<p>Also known as a "Talagrand diagram", this method checks where the verifying
observation usually falls with respect to the ensemble forecast data, which
is arranged in increasing order at each grid point. In an ensemble with
perfect spread, each member represents an equally likely scenario, so the
observation is equally likely to fall between any two members.
<p>To construct a rank histogram, do the following:
<br>1. At every observation (or analysis) point rank the <i>N</i> ensemble
members from lowest to highest. This represents N+1 possible bins that
the observation could fit into, including the two extremes
<br>2. Identify which bin the observation falls into at each point
<br>3. Tally over many observations to create a histogram of rank.
<p>Interpretation:
<br>Flat - ensemble spread about right to represent forecast uncertainty
<br>U-shaped - ensemble spread too small, many observations falling outside
the extremes of the ensemble
<br>Dome-shaped - ensemble spread too large, most observations falling
near the center of the ensemble
<br>Asymmetric - ensemble contains bias
<p>Note: A flat rank histogram does not necessarily indicate a good forecast,
it only measures whether the observed probability distribution is well
represented by the ensemble.
<p>
<hr WIDTH="100%">
<h3>
<i>References</i></h3>
<a NAME="Hamill 2001"></a>Hamill, T.M., 2001: Interpretation of rank histograms
for verifying ensemble forecasts. <i>Mon. Wea. Rev.</i>, <b>129</b>, 550-560.
<br><a NAME="Murphy 1973"></a>Murphy, A.H., 1973: A new vector partition
of the probability score. <i>J. Appl. Meteor.</i>,
<b>12</b>, 595-600.
<br><a NAME="Stanski et al 1989"></a>Stanski, H.R., L.J. Wilson, and W.R.
Burrows, 1989: Survey of common verification methods in meteorology. World
Weather Watch Tech. Rept. No.8, WMO/TD No.358, WMO, Geneva, 114 pp. Click
<a href="http://www.BoM.GOV.AU/bmrc/wefor/staff/eee/verif/Stanski_et_al/Stanski_et_al.html">here</a>
to access a PDF version.
<p>
<hr WIDTH="100%">
<p>Beth Ebert, BMRC Weather Forecasting Group, June 2003
</body>
</html>
